{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyAGF8fY_lcT"
      },
      "source": [
        "# Implementation of Hintons forward forward algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs__UyCk_lcU",
        "outputId": "11b6609a-ae16-4484-a8b8-8e80e2ef7966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "your student id: 99105901\n",
            "your name: Amirhossein Akbari\n"
          ]
        }
      ],
      "source": [
        "student_id =  99105901\n",
        "student_name = 'Amirhossein Akbari'\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRZ8kfBo_lcV"
      },
      "source": [
        "## Supervised fully connected network with pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfFU5PQf_lcW"
      },
      "source": [
        "### Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jLmzzPcj_lcW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda, GaussianBlur\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD2CIa4Q_lcW"
      },
      "source": [
        "### Functions to generate negative and positive data from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "h7hjYKQt_lcW"
      },
      "outputs": [],
      "source": [
        "def generate_positive(x, y):\n",
        "\n",
        "    overlayed = x.clone()\n",
        "    overlayed[:, :10] *= 0.0\n",
        "    overlayed[range(x.shape[0]), y] = 1\n",
        "\n",
        "    return overlayed\n",
        "\n",
        "\n",
        "def generate_negative(x, y):\n",
        "\n",
        "    y_neg = (y + torch.randint_like(y, 8) + 1) % 10\n",
        "\n",
        "    overlayed = x.clone()\n",
        "    overlayed[:, :10] *= 0.0\n",
        "    overlayed[range(x.shape[0]), y_neg] = 1\n",
        "\n",
        "    return overlayed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Obh-sg1_lcY"
      },
      "source": [
        "### Defining Layer and Network object implemented from nn.Linear layer and nn.Module network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With respect to repo. of Mohammad pezeshki and code as needed. I updated the model and added option to add layers and train them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Loss can meet the necessary conditions for goodness, if we write Cross Entropy Loss of layer assuming that we have equal number of neg. and pos. data; $ P(positive) = P(negative) $ , we can show that its equal to the defined loss in the question:\n",
        "$$ Loss = -p\\log{(\\sigma(\\sum_{j}^{} y_{pos_j}^2 - threshold))}-(1-p)\\log{(\\sigma(\\sum_{j}^{} y_{neg_j}^2 - threshold))} $$\n",
        "$$  = 0.5(\\log{(1+e^{-(\\sum_{j}^{} y_{pos_j}^2 - threshold)})}+\\log{(1+e^{-(\\sum_{j}^{} y_{neg_j}^2 - threshold)})}) $$\n",
        "$$  = mean[\\log{(1+e^{-(\\sum_{j}^{} y_{pos_j}^2 - threshold)})},\\log{(1+e^{-(\\sum_{j}^{} y_{neg_j}^2 - threshold)})}] $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9-9qXk51_lcY"
      },
      "outputs": [],
      "source": [
        "class FFLayer(nn.Linear):\n",
        "\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, device=None, dtype=None, th=2):\n",
        "\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=0.03)\n",
        "        self.threshold = th\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(torch.mm(x / (x.norm(2, 1, keepdim=True) + 1e-6), self.weight.T) + self.bias.unsqueeze(0)) # Forward with normalization L2\n",
        "\n",
        "    def train(self, x_pos, x_neg, num_epochs):\n",
        "\n",
        "        for _ in tqdm(range(num_epochs)):\n",
        "\n",
        "            gp = self.forward(x_pos).pow(2).mean(1)\n",
        "            gn = self.forward(x_neg).pow(2).mean(1)\n",
        "\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-gp + self.threshold, gn - self.threshold]))).mean()\n",
        "\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
        "\n",
        "\n",
        "class FFNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dims, use_cuda=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.dims = dims\n",
        "        self.using_cuda = use_cuda\n",
        "        self.layers = [FFLayer(dims[d], dims[d + 1]) for d in range(len(dims) - 1)]\n",
        "        if use_cuda:\n",
        "            self.layers = [l.cuda() for l in self.layers]\n",
        "\n",
        "\n",
        "    def add_layer(self, dim: int):\n",
        "        self.layers += [FFLayer(self.dims[-1], dim)]\n",
        "        self.dims += [dim]\n",
        "        if self.using_cuda:\n",
        "            self.layers[-1] = self.layers[-1].cuda()\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        g = []\n",
        "        for label in range(10):\n",
        "            h = generate_positive(x, label)             # Assuming the label is positive to see the neural activity\n",
        "            goodness = []\n",
        "\n",
        "            for l in self.layers:\n",
        "                h = l.forward(h)\n",
        "                goodness += [h.pow(2).mean(1)]\n",
        "\n",
        "            g += [sum(goodness).unsqueeze(1)]\n",
        "        g = torch.cat(g, 1)\n",
        "        return g.argmax(1)\n",
        "\n",
        "\n",
        "    def train(self, x_pos, x_neg, epochs_per_layer):\n",
        "\n",
        "        if self.using_cuda:\n",
        "            h_pos, h_neg = x_pos.cuda(), x_neg.cuda()\n",
        "        else:\n",
        "            h_pos, h_neg = x_pos, x_neg\n",
        "\n",
        "        for i, l in enumerate(self.layers):\n",
        "            print(f'training layer {i}')\n",
        "            h_pos, h_neg = l.train(h_pos, h_neg, epochs_per_layer[i])\n",
        "            l.trained = True\n",
        "\n",
        "        return h_pos.cpu(), h_neg.cpu()\n",
        "\n",
        "    def train_last(self, h_pos, h_neg, num_epochs):     # train one last layer of the model one more time(its defined to train added layers)\n",
        "        if self.using_cuda:\n",
        "            if h_pos.get_device() == -1:\n",
        "                h_pos = h_pos.cuda()\n",
        "            if h_neg.get_device() == -1:\n",
        "                h_neg = h_neg.cuda()\n",
        "\n",
        "        h_pos, h_neg = self.layers[-1].train(h_pos, h_neg, num_epochs)\n",
        "        return h_pos.cpu(), h_neg.cpu()\n",
        "\n",
        "    def accuracy(self, x, y): # Calculater accuracy of the model\n",
        "        return self.predict(x).eq(y).float().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LublNIpzDPE_"
      },
      "source": [
        "### Loading Dataset and training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZpzreAR_lcY",
        "outputId": "ba30feaf-f623-43ee-d2ff-55982bab822e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n",
            "training layer 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1300/1300 [01:14<00:00, 17.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 91.89%\n",
            "adding layer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [00:46<00:00, 25.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 92.71%\n",
            "train accuracy: 92.71%\n",
            "test accuracy: 92.67%\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Transform image into compatible vector\n",
        "transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0,), (1,)),\n",
        "        Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "# Load the whole dataset into tensor X and Y\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    MNIST('./data/', train=True,\n",
        "        download=True,\n",
        "        transform=transform),\n",
        "    batch_size=50000, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    MNIST('./data/', train=False,\n",
        "        download=True,\n",
        "        transform=transform),\n",
        "    batch_size=10000, shuffle=False)\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Using cuda' if use_cuda else 'Using cpu')\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "X_pos = generate_positive(x, y)\n",
        "X_neg = generate_negative(x, y)\n",
        "\n",
        "image_shape = X_pos.shape[1]\n",
        "\n",
        "model = FFNet([image_shape, 500], use_cuda)\n",
        "\n",
        "h_pos, h_neg = model.train(X_pos, X_neg, [1300])\n",
        "\n",
        "acc_last = 0\n",
        "acc = model.accuracy(x.cuda(), y.cuda())\n",
        "print('accuracy:', '%.2f' % (acc*100)+\"%\")\n",
        "\n",
        "while acc - acc_last > 0.01:\n",
        "    print('adding layer...')\n",
        "    model.add_layer(500)\n",
        "    h_pos, h_neg = model.train_last(h_pos, h_neg, 1200)\n",
        "    acc_last = acc\n",
        "    acc = model.accuracy(x.cuda(), y.cuda())\n",
        "    print('accuracy:', '%.2f' % (acc*100)+\"%\")\n",
        "\n",
        "print('train accuracy:', '%.2f' % (acc*100)+\"%\")\n",
        "\n",
        "x, y = next(iter(test_loader))\n",
        "\n",
        "print('test accuracy:', '%.2f' % (model.accuracy(x.cuda(), y.cuda())*100)+\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algorithm explanation\n",
        "\n",
        "The model above is just a nueral net that it loss is changed with another loss which is only dependent to goodness. the only difference with a EBP linear model is is train function where loss is being calculated and parameters being updated. in usual networks the loss is defined for each layer is dependent to all layers in forward toward the output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06V_qkUbDE3R"
      },
      "source": [
        "## Unsupervised fully connected network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHrtxQGeR2YK"
      },
      "source": [
        "### Updating data generator functions to use data in an unsupervised way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in ```generate_negative``` function, I used transforms to apply guassianBlur to a random tensor like X and applied a threshold as mentioned in article, I have tested the method below in mask_test.ipynb file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "3nXoriH9R2YK"
      },
      "outputs": [],
      "source": [
        "def generate_positive(x, _):\n",
        "    return x.clone()\n",
        "\n",
        "def generate_negative(x, _):\n",
        "\n",
        "    rnd = torch.randperm(x.size(0))\n",
        "\n",
        "    mask = torch.rand(x.shape[0], 28, 28)\n",
        "    transform = Compose([\n",
        "        GaussianBlur(5, 7),\n",
        "        GaussianBlur(7, 14),\n",
        "        GaussianBlur(5, 7),\n",
        "        Lambda(lambda x: x > 0.5)])\n",
        "    mask = transform(mask).view(x.shape[0], -1)\n",
        "\n",
        "    x_neg = x * mask + x[rnd] * ~mask\n",
        "\n",
        "    return x_neg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9TD-5JzR2YK"
      },
      "source": [
        "### Updating FFNet to add a linear classifier to its output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JzNkIT-sR2YL"
      },
      "outputs": [],
      "source": [
        "class FFNet2(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dims=[784, 2000, 2000, 2000, 2000], use_cuda=False):\n",
        "        super().__init__()\n",
        "        self.dims = dims\n",
        "        self.using_cuda = use_cuda\n",
        "        self.layers = [FFLayer(dims[d], dims[d + 1]) for d in range(len(dims) - 1)]\n",
        "        self.linear = self.linear = nn.Linear(sum(dims[2:]), 10)\n",
        "\n",
        "        if use_cuda:\n",
        "            self.layers = [l.cuda() for l in self.layers]\n",
        "            self.linear = self.linear.cuda()\n",
        "\n",
        "\n",
        "    def add_layer(self, dim: int):\n",
        "        self.layers += [FFLayer(self.dims[-1], dim)]\n",
        "        self.dims += [dim]\n",
        "        if self.using_cuda:\n",
        "            self.layers[-1] = self.layers[-1].cuda()\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        h = x\n",
        "        layer_outputs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer.forward(h)\n",
        "            if i > 0:\n",
        "                layer_outputs.append(h)\n",
        "\n",
        "        layer_outputs = torch.cat(layer_outputs, 1)\n",
        "\n",
        "        ans = self.linear(layer_outputs).argmax(1)\n",
        "\n",
        "        return ans\n",
        "\n",
        "\n",
        "    def train(self, x_pos, x_neg, y, num_epochs=[1000,10000]):\n",
        "\n",
        "        if self.using_cuda:\n",
        "            h_pos, h_neg = x_pos.cuda(), x_neg.cuda()\n",
        "            y = y.cuda()\n",
        "        else:\n",
        "            h_pos, h_neg = x_pos, x_neg\n",
        "\n",
        "        layer_outputs = []\n",
        "\n",
        "        for i, l in enumerate(self.layers):\n",
        "\n",
        "            print(f'training layer {i}')\n",
        "            h_pos, h_neg = l.train(h_pos, h_neg, num_epochs[0])\n",
        "            l.trained = True\n",
        "\n",
        "            if i > 0:\n",
        "                layer_outputs.append(h_pos)\n",
        "\n",
        "        layer_outputs = torch.cat(layer_outputs, 1)\n",
        "\n",
        "        print('training linear classifier')\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = Adam(model.linear.parameters(), lr=0.03)\n",
        "\n",
        "        for _ in range(num_epochs[1]):\n",
        "\n",
        "            output = model.linear(layer_outputs)\n",
        "            loss = criterion(output, y.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def accuracy(self, x, y):\n",
        "        return self.predict(x).eq(y).float().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading dataset and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-kvZX5vR2YL",
        "outputId": "e7da8d24-44a4-423b-bf6e-087d69bdd392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n",
            "training layer 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:56<00:00, 17.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training layer 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:39<00:00, 25.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training layer 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:40<00:00, 24.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training linear classifier\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Transform image into compatible vector\n",
        "transform = Compose([\n",
        "        ToTensor(),\n",
        "        Lambda(lambda x: torch.flatten(x)/255)])\n",
        "\n",
        "# Load the whole dataset into tensor X and Y\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    MNIST('./data/', train=True,\n",
        "        download=True,\n",
        "        transform=transform),\n",
        "    batch_size=50000, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    MNIST('./data/', train=False,\n",
        "        download=True,\n",
        "        transform=transform),\n",
        "    batch_size=10000, shuffle=False)\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Using cuda' if use_cuda else 'Using cpu')\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "X_pos = generate_positive(x, y)\n",
        "X_neg = generate_negative(x, y)\n",
        "\n",
        "image_shape = X_pos.shape[1]\n",
        "\n",
        "model = FFNet2(dims=[784, 500, 500, 500], use_cuda=use_cuda)\n",
        "\n",
        "model.train(X_pos, X_neg, y, num_epochs=[1100, 2000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNsRWvPgcMX3",
        "outputId": "f8fa5b97-d24e-40dc-b488-e6cf7b288c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train accuracy: 95.67\n",
            "test accuracy: 92.19\n"
          ]
        }
      ],
      "source": [
        "\n",
        "acc = model.accuracy(x.cuda(), y.cuda())\n",
        "\n",
        "print('train accuracy:', '%.2f' % (acc*100))\n",
        "\n",
        "x, y = next(iter(test_loader))\n",
        "\n",
        "print('test accuracy:', '%.2f' % (model.accuracy(x.cuda(), y.cuda())*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model outputs\n",
        "\n",
        "The output of the first part of the model is a large vector called ```layer_outputs```, which contains a large number of features from the input data. Model neurons are trained to react to real numbers that represent one of the digits, but they do not react to meaningless images that have similarities and short-range correlation with digits. So, the model has learned to extract features from the images that contain a real number, and in this case, some neurons will definitely react more to some specific digits.\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "we just added a linear classifier layer to the model and passed the output of all neurons (except first layer as mentioned in the article) to it so as mentioned before it can learn how to distinguish features from each other with sufficient epochs of training."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
